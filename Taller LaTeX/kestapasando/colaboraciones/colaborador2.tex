Para finalizar veremos un algoritmo en sumo útil para generar variables aleatorias continuas con solo conocer un múltiplo de la función de densidad: el \emph{método de aceptación y rechazo}. Supongamos que $f$ es una función de densidad y que $g$ es otra función de densidad tal que $f \leq M g$ para alguna $M > 0$. El algoritmo consiste en los siguientes pasos:
\begin{enumerate}
    \item Generamos una variable aleatoria $X$ de la función de densidad $g$ y una variable aleatoria $U \sim \unif(0,1)$.
    \item Si $U \leq f(X) / (M g(X))$, entonces definimos $Y = X$ y terminamos.
    \item En otro regresamos a 1.
\end{enumerate}

\begin{proposition} \label{prop:aceptacion y rechazo}
    La variable aleatoria $Y$ generada mediante el algoritmo de aceptación y rechazo tiene densidad $f$. Más aún, la cantidad de simulaciones que haremos para aceptar un valor $X$ es una variable aleatoria geométrica con parámetro $1/M$.
\end{proposition}

\begin{proof}
    Obtengamos la función de distribución de $Y$. 
    \begin{align*}
        \PP(Y \leq y) & = \PP \left( X \leq y \,\Bigg\vert\, U \leq \frac{f(X)}{M g(X)} \right) 
        = \frac{\PP \left( X \leq y, U \leq \frac{f(X)}{Mg(X)} \right)}{\PP \left(U \leq \frac{f(X)}{Mg(X)} \right)}.
    \end{align*}
    Para el numerador tendremos, por independencia,
    \begin{align*}
        \PP \left( X \leq y, U \leq \frac{f(X)}{Mg(X)} \right) & = \int_{-\infty}^y \PP\left(U \leq \frac{f(x)}{Mg(x)}\right) g(x) \dd x = \int_{-\infty}^y g(x) \int_0^{f(x)/Mg(x)} \dd u \dd x \\
        & = \int_{-\infty}^y g(x) \frac{f(x)}{Mg(x)} \dd x = \frac 1 M \int_{-\infty}^y f(x) \dd x,
    \end{align*}
    mientras que el denominador estará dado por 
    \begin{align}
        \PP \left(U \leq \frac{f(X)}{Mg(X)} \right) & = \lim_{y \to \infty} \frac 1 M \int_{-\infty}^y f(x) \dd x = \frac 1 M \int_{-\infty}^\infty f(x) \dd x = \frac 1 M. \label{eq:geometrica}
    \end{align}
    Así las cosas, $\PP(Y \leq y) = \int_{-\infty}^y f(x) \dd x$ y por lo tanto $f_Y(y) = f(y)$. Más aún, de (\ref{eq:geometrica}) vemos que la probabilidad de aceptar una simulación $X$ como simulación de $Y$ es $1/M$, de donde se sigue fácilmente el resultado.
\end{proof}

Una consecuencia importante de la demostración de la proposición \ref{prop:aceptacion y rechazo} es que en realidad no es necesario conocer $f$ en su totalidad, basta conocer un múltiplo de la función de densidad. Esto es útil en estadística bayesiana, pues para un valor de $x$ fijo se tiene la relación de proporcionalidad
\[
    f_{\Theta \,\vert\, X}(\theta \,\vert\, x) \propto f_{X \,\vert\, \Theta}(x \,\vert\, \theta) f_\Theta(\theta) = f_{X, \Theta}(x, \theta),
\]
por lo cual para realizar simulaciones de la \emph{distribución posterior} de $\Theta$, basta conocer la densidad conjunta y no hará falta normalizar. El costo a pagar será encontrar una función de densidad $g$ y una $M > 0$ tales que $f_{X, \Theta}(x, \theta) \leq M g(\theta)$ para todo $\theta$.

\begin{example}[Simulación de gaussianas estándar de una Cauchy]
    Cuando $f(x) = (2\pi)^{-1/2} \exp(-x^2/2)$ y $g(x) = (\pi(1+x^2))^{-1}$,
    \[
        \frac{f(x)}{g(x)} = \sqrt\frac{\pi}{2} (1+x^2) e^{-\frac{x^2}{2}}
    \]
    está acotado por $M = \sqrt{2\pi/e}$, valor que se alcanza en $x = \pm 1$, por lo que la probabilidad de aceptación es $1/M = \sqrt{e/2\pi} \approx 0.66$, lo cual implica que en promedio, una de cada tres simulaciones de la Cauchy es rechazada.
\end{example}

\begin{example}[Generación de variables aleatorias gamma]
    Supongamos que $\alpha \geq 1$ y, sin pérdida de generalidad, supongamos que $\lambda = 1$. Entonces $f(x) = x^{\alpha - 1} e^{-x} / \Gamma(\alpha)$. Proponemos $g_b(x) = b^a x^{a-1} e^{-bx} / \Gamma(a)$, con $a = \lfloor \alpha \rfloor$ y $b < 1$. Entonces 
    \[
        \frac{f(x)}{g_b(x)} = \frac{\Gamma(a)}{\Gamma(\alpha)} b^{-a} x^{\alpha - a} e^{-(1-b)x} \leq \frac{\Gamma(a)}{\Gamma(\alpha)} b^{-a} \left( \frac{\alpha-a}{(1-b)e} \right)^{\alpha-a},
    \]
    y vemos que la cota mínima se alcanza con $b = a / \alpha$.
\end{example}